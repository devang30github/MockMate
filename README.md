# ğŸ¤ MockMate-AI Mock Interview Assistant

An AI-powered voice-based interview simulation platform that evaluates user responses and generates a detailed PDF report â€” built with Flask, LangChain, Whisper, and ChromaDB.

---

## ğŸš€ Features

- Upload your resume (PDF)
- Auto-generated mock interview questions (technical, behavioral, and role-specific)
- Voice-based Q&A with real-time transcription using OpenAI Whisper
- AI evaluation of answers with scores, feedback, and categories
- Interactive frontend with progress tracking
- Final PDF report with scores, feedback, and summary

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ app.py                        # Flask backend with API endpoints
â”œâ”€â”€ mock_interview_agent.py      # Core logic: resume parsing, interview generation, evaluation
â”œâ”€â”€ main.js                      # Frontend interaction logic (audio recording, session control)
â”œâ”€â”€ index.html                   # UI template (Bootstrap based)
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html               # HTML entry point for Flask
â”œâ”€â”€ static/
â”‚   â””â”€â”€ main.js                  # Frontend JavaScript
â”œâ”€â”€ uploads/                     # Uploaded resumes
â”œâ”€â”€ reports/                     # Generated PDF reports
â”œâ”€â”€ chroma_db/                   # Resume vector DB (auto-generated)
â””â”€â”€ eval_db/                     # Evaluation vector DB (auto-generated)
```

---

## ğŸ› ï¸ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/mock-interview-assistant.git
cd mock-interview-assistant
```

### 2. Create Virtual Environment & Install Dependencies

```bash
python -m venv venv
source venv/bin/activate   # On Windows use venv\Scripts\activate
pip install -r requirements.txt
```

### 3. Setup Environment Variables

Create a `.env` file:

```
OPENROUTER_API_KEY=your_openrouter_key
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
HF_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
```

### 4. Install FFmpeg

Download and install FFmpeg, then add it to your system PATH.

ğŸ“½ï¸ [Install guide](https://youtu.be/GYdhqmy_Nt8?si=6Rmr8Po4vWqNTFYg)

---

## â–¶ï¸ Running the App

```bash
python app.py
```

Visit [http://localhost:5000](http://localhost:5000) in your browser.

---

## ğŸ§  Powered By

- ğŸ§© **LangChain + LangGraph** â€” dynamic agent workflows
- ğŸ—‚ï¸ **ChromaDB** â€” vector storage for resumes and evaluations
- ğŸ§  **Whisper** â€” real-time audio transcription
- ğŸ“„ **ReportLab** â€” export structured PDF reports
- ğŸ™ï¸ **Pyttsx3 & Pyaudio** â€” voice input/output

---

## ğŸ“Œ Future Improvements

- Role selection before question generation
- Scoring visualization (e.g., radar/spider charts)
- Support for multilingual interviews
- Option to switch to FastAPI backend

---

## ğŸ“ƒ License

MIT License â€” free to use and modify!

---

## âœ¨ Demo Screenshot

![screenshot](https://user-images.githubusercontent.com/demo-screenshot.png) <!-- Replace with actual image if available -->

---

## ğŸ™Œ Contribute

Pull requests welcome! Please open an issue first if you want to add a feature or fix a bug.
